import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import random

# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œï¼‰
df = pd.read_csv('market_data.csv', 
                 names=['timestamp', 'symbol', 'imbalance', 'diff', 'volume', 'entry_price', 'exit_price', 'price_change'],
                 header=0)

# 2. éŠ˜æŸ„ã”ã¨ã«åˆ†å‰²
target_symbol = 'ATOMUSDT'   # C++ã® target ã¨åˆã‚ã›ã‚‹
support_symbol = 'BTCUSDT'  # C++ã® support ã¨åˆã‚ã›ã‚‹

target_df = df[df['symbol'] == target_symbol].reset_index(drop=True)
support_df = df[df['symbol'] == support_symbol].reset_index(drop=True)

# æ¬ æè€ƒæ…®ï¼šæœ€å°ã®é•·ã•ã«åˆã‚ã›ã‚‹
min_len = min(len(target_df), len(support_df))

# 3. æ¨ªæŒã¡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
# â€»ã“ã® features ã®é †ç•ªãŒ C++ ã® input é…åˆ—ã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
features = ['target_imb', 'target_diff', 'target_vol', 'support_imb', 'support_diff', 'support_vol']
combined_data = {
    'target_imb':  target_df.loc[:min_len-1, 'imbalance'].values,
    'target_diff': target_df.loc[:min_len-1, 'diff'].values,
    'target_vol':  target_df.loc[:min_len-1, 'volume'].values,
    'support_imb': support_df.loc[:min_len-1, 'imbalance'].values,
    'support_diff':support_df.loc[:min_len-1, 'diff'].values,
    'support_vol': support_df.loc[:min_len-1, 'volume'].values,
    'price_change':target_df.loc[:min_len-1, 'price_change'].values # targetã®å¤‰åŒ–ã‚’äºˆæ¸¬å¯¾è±¡ã«ã™ã‚‹
}

combined_df = pd.DataFrame(combined_data)
# ç›´è¿‘20,000ã€œ30,000è¡Œã«çµã‚‹ï¼ˆç›¸å ´ã®ã€Œä»Šã€ã«ç‰¹åŒ–ã•ã›ã‚‹ï¼‰
combined_df = combined_df.tail(30000).reset_index(drop=True)

# 4. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler = MinMaxScaler()
df_scaled = combined_df.copy()
df_scaled[features] = scaler.fit_transform(combined_df[features])

# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
DATA = df_scaled[features].values
PRICE_CHANGES = df_scaled['price_change'].values
num_features = len(features)
width = high = 20
# DATAã®ä¸­ã‹ã‚‰ã€ãƒãƒ¼ãƒ‰ã®æ•°ï¼ˆ400å€‹ï¼‰ã ã‘ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡º
#SOMã®å­¦ç¿’ã‚’å§‹ã‚ã‚‹éš›ã€åœ°å›³ã®åˆæœŸçŠ¶æ…‹ï¼ˆåˆæœŸã®é‡ã¿ï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ±ºã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚çœŸã£ç™½ãªåœ°å›³ã‹ã‚‰å§‹ã‚ã‚‹ã‚ˆã‚Šã€**ã€Œä»Šã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã„ãã¤ã‹é¸ã‚“ã§åœ°å›³ã®åˆæœŸå€¤ã«ã™ã‚‹ã€**ã»ã†ãŒå­¦ç¿’ãŒé€Ÿãé€²ã‚€ãŸã‚ã€ã“ã®æ‰‹æ³•ãŒã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚
images_data = np.array(random.sample(DATA.tolist(), width * high))


#æš´éœ²
epochs = 50
initial_ste = 0.05    # åˆæœŸã®å­¦ç¿’ç‡ï¼ˆå¼·ã‚ï¼‰
initial_sigma = 4.0  # åˆæœŸã®è¿‘å‚åŠå¾„ï¼ˆåºƒã‚ï¼‰
for epoch in range(epochs):
	#print(f"Epoch {epoch+1}/{epochs} starting...")
    # ã‚°ãƒªãƒƒãƒ‰ã®åº§æ¨™è¡Œåˆ—ã‚’äº‹å‰ã«ä½œæˆ (20x20)
	x, y = np.meshgrid(np.arange(width), np.arange(high))
	grid_coords = np.stack([x.flatten(), y.flatten()], axis=1)

	for i in range(len(DATA)):
		# å­¦ç¿’ç‡ã¨åŠå¾„ã‚’ã€å¾ŒåŠã«è¡Œãã»ã©å°ã•ãã™ã‚‹
		# å…¨ä½“ã®é€²è¡Œåº¦ï¼ˆã‚¨ãƒãƒƒã‚¯ã‚‚è€ƒæ…®ï¼‰ã§å­¦ç¿’ç‡ã‚’æ¸›è¡°ã•ã›ã‚‹
        # total_progress ã¯ 0.0 ã‹ã‚‰ 1.0 ã¾ã§é€²ã‚€
		total_progress = (epoch * len(DATA) + i) / (epochs * len(DATA))
		current_ste = initial_ste * (1.0 - total_progress)
		current_sigma = initial_sigma * (1.0 - total_progress)
		
		# 1. BMU (Winner) ã‚’æ¢ã™ (é«˜é€Ÿç‰ˆ)
		diffs = np.array(images_data) - DATA[i]
		dist_sqs = np.sum(diffs**2, axis=1)
		winner_idx = np.argmin(dist_sqs)
		
		# 2. å…¨ãƒãƒ¼ãƒ‰ã¨BMUã®è·é›¢ã‚’ä¸€æ°—ã«è¨ˆç®—
		bmu_coord = grid_coords[winner_idx]
		dist_to_bmu = np.sum((grid_coords - bmu_coord)**2, axis=1)
		
		# 3. ã‚¬ã‚¦ã‚¹è¿‘å‚é–¢æ•°ã‚’ä¸€æ‹¬é©ç”¨
		influence = np.exp(-dist_to_bmu / (2 * (current_sigma**2 + 1e-5)))
		
		# 4. é‡ã¿ã‚’ä¸€æ‹¬æ›´æ–° (images_data: [400, 6], influence: [400, 1])
		images_data += (DATA[i] - images_data) * (current_ste * influence[:, np.newaxis])

# --- æœ€çµ‚çµæœï¼šä¾¡æ ¼å¤‰åŒ–ã®ã‚°ãƒ©ãƒ‡ãƒãƒƒãƒ— ---
node_weights = np.zeros(width * high)
node_counts = np.ones(width * high) * 0.001

for i in range(len(DATA)):
    dists = np.sum(np.abs(np.array(images_data) - DATA[i]), axis=1)
    winner = np.argmin(dists)
    node_weights[winner] += PRICE_CHANGES[i]
    node_counts[winner] += 1

avg_change = node_weights / node_counts

# å¯†åº¦ãƒãƒƒãƒ—ï¼ˆãƒ‡ãƒ¼ã‚¿ã®å‡ºç¾å›æ•°ï¼‰ã‚’æ­£è¦åŒ–
density = node_counts.reshape(width, high)
density_norm = (density - density.min()) / (density.max() - density.min())

plt.figure(figsize=(10, 8))
# èƒŒæ™¯ã«ä¾¡æ ¼å¤‰åŒ–ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
img = plt.imshow(avg_change.reshape(width, high), cmap='RdYlGn', interpolation='nearest')
# å¯†åº¦ã‚’ç­‰é«˜ç·šï¼ˆContourï¼‰ã¨ã—ã¦é‡ã­ã‚‹
plt.contour(density_norm, colors='black', alpha=0.5, levels=5) 
plt.colorbar(img, label='Avg Price Change')
plt.title('Strategy Map with Density Contours')
plt.xticks(range(0, width, 2)); plt.yticks(range(0, high, 2))
#plt.show()

# 5. ã‚·ã‚°ãƒŠãƒ«æŠ½å‡º
node_weights = np.zeros(width * high)
node_counts = np.ones(width * high) * 0.001
for i in range(len(DATA)):
    dists = np.sum(np.abs(np.array(images_data) - DATA[i]), axis=1)
    winner = np.argmin(dists)
    node_weights[winner] += PRICE_CHANGES[i]
    node_counts[winner] += 1
avg_change = node_weights / node_counts

# ã‚·ã‚°ãƒŠãƒ«å®šç¾©
final_expectancy = np.where(node_counts >= 0.5, avg_change, 0.0)
# å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ä¸€æ‹¬ã§ BMU (Winner index) ã‚’æ±‚ã‚ã‚‹
print("Calculating BMUs for all data...")
all_dists = np.linalg.norm(images_data[:, np.newaxis] - DATA, axis=2)
all_winners = np.argmin(all_dists, axis=0)

# å„ãƒãƒ¼ãƒ‰ã«å±ã™ã‚‹ä¾¡æ ¼å¤‰åŒ–ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
node_to_changes = [[] for _ in range(width * high)]
for i, winner in enumerate(all_winners):
    node_to_changes[winner].append(PRICE_CHANGES[i])

# å„ãƒãƒ¼ãƒ‰ã®çµ±è¨ˆé‡ã‚’è¨ˆç®—
final_expectancy = np.zeros(width * high)
node_std = np.zeros(width * high)

for idx in range(width * high):
    changes = node_to_changes[idx]
    if len(changes) > 0:
        final_expectancy[idx] = np.mean(changes)
        if len(changes) > 1:
            node_std[idx] = np.std(changes)
        else:
            node_std[idx] = 0.05
    else:
        final_expectancy[idx] = 0.0
        node_std[idx] = 0.05

# ã¾ã¨ã‚ã¦ä¿å­˜
np.savetxt("som_expectancy.csv", final_expectancy, delimiter=",")
np.savetxt("som_risk_map.csv", node_std, delimiter=",")

# 6. C++ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
# åœ°å›³ã®é‡ã¿
np.savetxt("som_map_weights.csv", np.array(images_data), delimiter=",")
# ã‚·ã‚°ãƒŠãƒ«
# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°
scaling_params = pd.DataFrame({
    'min': scaler.data_min_,
    'max': scaler.data_max_
})
scaling_params.to_csv("som_scaling_params.csv", index=False)

print("C++ç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å®Œäº†ï¼")

# 7. æ¬¡å›ã®èª­ã¿è¾¼ã¿ã‚’é€Ÿãã™ã‚‹ãŸã‚ã«ã€æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
# æœ€æ–°ã®5ä¸‡è¡Œç¨‹åº¦ã‚’æ®‹ã—ã¦ãŠã‘ã°ã€æ¬¡å›ã®å­¦ç¿’ï¼ˆ3ä¸‡è¡Œä½¿ç”¨ï¼‰ã«ã¯ååˆ†ã§ã™
keep_limit = 50000
if len(df) > keep_limit:
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å«ã‚ã¦æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ã
    df.tail(keep_limit).to_csv('market_data.csv', index=False)
    print(f"ğŸ§¹ Market data cleaned. Kept latest {keep_limit} rows.")